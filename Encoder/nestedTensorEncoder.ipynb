{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nested import nested_tensor\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "max_seq_len = 100\n",
    "embed_dim = 768\n",
    "num_batches = 200\n",
    "\n",
    "\n",
    "seq_lengths = torch.randint(10, max_seq_len + 1, (num_batches, batch_size))\n",
    "base_batches = [\n",
    "    [torch.randn(seq_len, embed_dim) for seq_len in batch_seq_lengths]\n",
    "    for batch_seq_lengths in seq_lengths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_batches = [nested_tensor(batch) for batch in base_batches]\n",
    "\n",
    "\n",
    "class TransformerEncoderWithNested(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerEncoderWithNested, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, nested_input):\n",
    "        outputs = []\n",
    "        for tensor in nested_input:\n",
    "            attn_output, _ = self.attention(tensor, tensor, tensor)\n",
    "            tensor = self.norm1(tensor + attn_output)\n",
    "            ff_output = self.ff(tensor)\n",
    "            tensor = self.norm2(tensor + ff_output)\n",
    "            outputs.append(tensor)\n",
    "        return nested_tensor(outputs)\n",
    "\n",
    "    \n",
    "    \n",
    "model_nested = TransformerEncoderWithNested(embed_dim=embed_dim, num_heads=8, ff_dim=256).to(device)\n",
    "\n",
    "forward_times_nested = []\n",
    "for nested_batch in tqdm(nested_batches, total=num_batches):\n",
    "    nested_batch = nested_batch.to(device)\n",
    "    start_time = time.time()\n",
    "    output = model_nested(nested_batch)\n",
    "    forward_times_nested.append(time.time() - start_time)\n",
    "\n",
    "print(f\"Forward pass time with NestedTensors: {sum(forward_times_nested)/len(forward_times_nested):.6f} seconds/batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"nested_time.npy\", np.array(forward_times_nested))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
