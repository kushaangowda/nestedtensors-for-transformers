{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nested import nested_tensor\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "max_seq_len = 100\n",
    "embed_dim = 768\n",
    "num_batches = 200\n",
    "\n",
    "\n",
    "seq_lengths = torch.randint(10, max_seq_len + 1, (num_batches, batch_size))\n",
    "base_batches = [\n",
    "    [torch.randn(seq_len, embed_dim) for seq_len in batch_seq_lengths]\n",
    "    for batch_seq_lengths in seq_lengths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_batches = []\n",
    "attention_masks = []\n",
    "\n",
    "for batch in base_batches:\n",
    "    batch_padded = []\n",
    "    mask = []\n",
    "    for sequence in batch:\n",
    "        seq_len = sequence.size(0)\n",
    "        padded_sequence = torch.cat([sequence, torch.zeros(max_seq_len - seq_len, embed_dim)], dim=0)\n",
    "        batch_padded.append(padded_sequence)\n",
    "        mask.append([0] * seq_len + [1] * (max_seq_len - seq_len))\n",
    "    padded_batches.append(torch.stack(batch_padded))\n",
    "    attention_masks.append(torch.tensor(mask, dtype=torch.bool))\n",
    "\n",
    "    \n",
    "class TransformerEncoderWithPadding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerEncoderWithPadding, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.attention(x, x, x, key_padding_mask=attention_mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x.transpose(0, 1)\n",
    "\n",
    "    \n",
    "    \n",
    "model_padded = TransformerEncoderWithPadding(embed_dim=embed_dim, num_heads=8, ff_dim=256).to(device)\n",
    "\n",
    "forward_times_padded = []\n",
    "for padded_batch, mask in tqdm(zip(padded_batches, attention_masks), total=num_batches):  \n",
    "    padded_batch = padded_batch.to(device)\n",
    "    mask = mask.to(device)\n",
    "    start_time = time.time()\n",
    "    output = model_padded(padded_batch, mask)\n",
    "    forward_times_padded.append(time.time() - start_time)\n",
    "\n",
    "print(f\"Forward pass time with padding: {sum(forward_times_padded)/len(forward_times_padded):.6f} seconds/batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"padding_time.npy\", np.array(forward_times_padded))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
